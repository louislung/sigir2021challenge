{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, activations, losses, Model, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy, SparseTopKCategoricalAccuracy\n",
    "# import tensorflow_ranking as tfr\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os, time, gc, json\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from matplotlib import pyplot as plt\n",
    "from joblib import Parallel, delayed\n",
    "from ast import literal_eval\n",
    "import multiprocessing\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_shift_features(df, c, lags=[1,2,3,4,5]):\n",
    "    try:\n",
    "        naid = df[c[:-3]].cat.categories.tolist().index('_')\n",
    "    except ValueError:\n",
    "        naid = df[c].max() + 1\n",
    "    \n",
    "    for i in lags:\n",
    "        if (c + '_lag%d'%i) in df.columns:\n",
    "            continue\n",
    "        tmp = df[['session_id_hash', c]].shift(i)\n",
    "        tmp.loc[df.session_id_hash != tmp.session_id_hash, c] = naid   \n",
    "        tmp[c] = tmp[c].astype(df[c].dtype)\n",
    "        df[c + '_lag%d'%i] = tmp[c]\n",
    "        print('Created\\t' + c + '_lag%d'%i)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read training data\n",
    "df_browse = pd.read_csv(\"./data/browsing_train.csv\")\n",
    "df_browse['train'] = np.int8(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-15 05:02:44.513000\n",
      "2019-04-15 03:59:58.560000\n"
     ]
    }
   ],
   "source": [
    "df_browse.server_timestamp_epoch_ms = pd.to_datetime(df_browse.server_timestamp_epoch_ms, unit='ms')\n",
    "df_browse['server_day'] = df_browse.server_timestamp_epoch_ms.dt.date\n",
    "print(df_browse.server_timestamp_epoch_ms.min())\n",
    "print(df_browse.server_timestamp_epoch_ms.max())\n",
    "val_session_id = df_browse.session_id_hash[df_browse.server_day > pd.to_datetime('2019-03-20')].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1961.2509031295776"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_browse.memory_usage().sum()/1024**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-15 04:00:00.991000\n",
      "2019-05-14 03:57:58.517000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "49596"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read testing data\n",
    "with open('./data/rec_test_phase_1.json') as f:\n",
    "    df_test = json.load(f)\n",
    "df_test = pd.json_normalize(df_test, record_path =['query'])\n",
    "df_test['train'] = np.int8(0)\n",
    "# todo: should take search event into account\n",
    "df_test = df_test[~df_test.is_search]\n",
    "\n",
    "df_test.server_timestamp_epoch_ms = pd.to_datetime(df_test.server_timestamp_epoch_ms, unit='ms')\n",
    "df_test['server_day'] = df_test.server_timestamp_epoch_ms.dt.date\n",
    "print(df_test.server_timestamp_epoch_ms.min())\n",
    "print(df_test.server_timestamp_epoch_ms.max())\n",
    "\n",
    "df_test = df_test[df_browse.columns]\n",
    "assert df_test.session_id_hash.isin(df_browse.session_id_hash).sum() == 0\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set items which only appears in test set as minority\n",
    "for c in ['product_sku_hash', 'hashed_url']:\n",
    "    tmp = df_test[c][~df_test[c].isna()]\n",
    "    test_only_sku = tmp[~tmp.isin(df_browse[c])]\n",
    "    df_test.loc[df_test[c].isin(test_only_sku), c] = 'minority'\n",
    "    del tmp, test_only_sku\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1991.6658773422241"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine training and testing data\n",
    "df_browse = pd.concat((df_browse, df_test), ignore_index=True)\n",
    "df_browse.sort_values(['session_id_hash', 'server_timestamp_epoch_ms'], inplace=True, ignore_index=True)\n",
    "del df_test\n",
    "gc.collect()\n",
    "df_browse.memory_usage().sum()/1024**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine values with few records in training\n",
    "for c in ['product_sku_hash', 'hashed_url']:\n",
    "    tmp = df_browse[c].value_counts()\n",
    "    df_browse.loc[df_browse[c].isin(tmp[tmp<=1].index), c] = 'minority'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# factorize columns\n",
    "for c in ['product_sku_hash', 'hashed_url', 'event_type', 'product_action']:\n",
    "    # set NaN to _\n",
    "    if df_browse[c].isnull().values.any():\n",
    "        df_browse.loc[df_browse[c].isna(), c] = '_'\n",
    "    # add new column, id start from 0\n",
    "    df_browse[c] = df_browse[c].astype('category')\n",
    "    df_browse[c + '_id'] = df_browse[c].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1583.5668745040894"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_browse.memory_usage().sum()/1024**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created\tproduct_sku_hash_id_lag1\n",
      "Created\tproduct_sku_hash_id_lag2\n",
      "Created\tproduct_sku_hash_id_lag3\n",
      "Created\tproduct_sku_hash_id_lag4\n",
      "Created\tproduct_sku_hash_id_lag5\n",
      "\n",
      "Created\thashed_url_id_lag1\n",
      "Created\thashed_url_id_lag2\n",
      "Created\thashed_url_id_lag3\n",
      "Created\thashed_url_id_lag4\n",
      "Created\thashed_url_id_lag5\n",
      "\n",
      "Created\tevent_type_id_lag1\n",
      "Created\tevent_type_id_lag2\n",
      "Created\tevent_type_id_lag3\n",
      "Created\tevent_type_id_lag4\n",
      "Created\tevent_type_id_lag5\n",
      "\n",
      "Created\tproduct_action_id_lag1\n",
      "Created\tproduct_action_id_lag2\n",
      "Created\tproduct_action_id_lag3\n",
      "Created\tproduct_action_id_lag4\n",
      "Created\tproduct_action_id_lag5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# add shift features\n",
    "max_lag = 5\n",
    "for c in ['product_sku_hash_id', 'hashed_url_id', 'event_type_id', 'product_action_id']:\n",
    "    create_shift_features(df_browse, c, lags=[i+1 for i in range(max_lag)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add continuous features\n",
    "# df_browse['sess_step'] = df_browse.groupby('session_id_hash').cumcount().astype(np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created\tproduct_sku_hash_id_lag-1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# add target features (this column only useful for train set)\n",
    "create_shift_features(df_browse, 'product_sku_hash_id', lags=[-1])\n",
    "df_browse.rename(columns={'product_sku_hash_id_lag-1': 'next_sku'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naid = df_browse.product_sku_hash.cat.categories.tolist().index('_')\n",
    "minorid = df_browse.product_sku_hash.cat.categories.tolist().index('minority')\n",
    "\n",
    "# add next_interacted_sku\n",
    "df_browse['next_interacted_sku'] = df_browse.next_sku.copy()\n",
    "df_browse.loc[df_browse.next_sku==naid, 'next_interacted_sku'] = np.nan\n",
    "df_browse.next_interacted_sku = df_browse.groupby('session_id_hash')['next_interacted_sku'].apply(lambda x: x.bfill())\n",
    "df_browse.loc[df_browse.next_interacted_sku.isna(), 'next_interacted_sku'] = naid\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set kfold based on date\n",
    "df_browse['kfoldidx'] = 1\n",
    "df_browse.loc[df_browse.session_id_hash.isin(val_session_id), 'kfoldidx'] = 0\n",
    "df_browse.loc[df_browse.train==0, 'kfoldidx'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a random id per group, used for calculating testing metric later\n",
    "df_browse = df_browse.sample(frac=1, random_state=0).reset_index(drop=True)\n",
    "df_browse['rand_id'] = df_browse.groupby('session_id_hash').cumcount().astype(np.int16)\n",
    "df_browse.sort_values(['session_id_hash', 'server_timestamp_epoch_ms'], inplace=True, ignore_index=True)\n",
    "\n",
    "# pick a row per group with min rand_id\n",
    "# df_browse.head(1000)[df_browse.head(1000).groupby(['session_id_hash'])['rand_id'].transform(min) == df_browse.head(1000)['rand_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_browse.to_parquet('df_browse_v2_update_0516')\n",
    "\n",
    "# can simply reuse df_browse_v3_update\n",
    "df_browse = pd.read_parquet('df_browse_v3_update')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_browse['sample_weights'] = pd.to_datetime(df_browse.server_day).dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get num_x from x_lag1 because function create_shift_features\n",
    "# may created a new_id in lag columns\n",
    "num_sku = df_browse.product_sku_hash_id_lag1.max() + 1\n",
    "num_url = df_browse.hashed_url_id_lag1.max() + 1\n",
    "# num_action = df_browse.product_action_id_lag1.max() + 1\n",
    "# num_event = df_browse.event_type_id_lag1.max() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_prod = ['product_sku_hash_id_lag5', 'product_sku_hash_id_lag4', 'product_sku_hash_id_lag3', \n",
    "            'product_sku_hash_id_lag2', 'product_sku_hash_id_lag1', 'product_sku_hash_id']\n",
    "fea_url = ['hashed_url_id_lag5', 'hashed_url_id_lag4', 'hashed_url_id_lag3', \n",
    "           'hashed_url_id_lag2', 'hashed_url_id_lag1', 'hashed_url_id']\n",
    "# fea_event = ['event_type_id_lag5', 'event_type_id_lag4', 'event_type_id_lag3',\n",
    "#              'event_type_id_lag2', 'event_type_id_lag1', 'event_type_id']\n",
    "# fea_action = ['product_action_id_lag5', 'product_action_id_lag4', 'product_action_id_lag3', 'product_action_id_lag2',\n",
    "#               'product_action_id_lag1', 'product_action_id',]\n",
    "# fea_step = ['sess_step']\n",
    "\n",
    "features = fea_prod + fea_url # + fea_event + fea_action + fea_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naid = df_browse.product_sku_hash.cat.categories.tolist().index('_')\n",
    "minorid = df_browse.product_sku_hash.cat.categories.tolist().index('minority')\n",
    "\n",
    "df_train = df_browse[df_browse.train==1]\n",
    "df_train = df_train[(df_train.next_sku!=naid)].reset_index(drop=True)\n",
    "\n",
    "df_test = df_browse[df_browse.train==0]\n",
    "df_test = df_test[(df_test.next_sku!=naid)].reset_index(drop=True)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop(columns='kfoldidx', inplace=True)\n",
    "df_test['kfoldidx'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    1870935\n",
      "2    1869633\n",
      "3    1869217\n",
      "4    1867049\n",
      "0    1866106\n",
      "Name: kfoldidx, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def strafiedkfold(df, idcol, k=5):\n",
    "    \"\"\"\n",
    "    sklearn kfold will do unnessary sorting\n",
    "    so build my own function\n",
    "    \"\"\"\n",
    "    df_kidx = df_train.session_id_hash.unique()\n",
    "    np.random.seed(123)\n",
    "    np.random.shuffle(df_kidx)\n",
    "    df_kidx = pd.DataFrame({idcol: df_kidx})\n",
    "    df_kidx['kfoldidx'] = df_kidx.index % k\n",
    "    df = df.merge(df_kidx, on=idcol, copy=False)\n",
    "    print(df['kfoldidx'].value_counts())\n",
    "    return df\n",
    "\n",
    "df_train = strafiedkfold(df_train, 'session_id_hash', k=5)\n",
    "df_train = pd.concat([df_train, df_test])\n",
    "x_train = []\n",
    "x_val = []\n",
    "x_one = []\n",
    "for f in [fea_prod, fea_url]:\n",
    "    x_train.append(np.array(df_train.loc[df_train.kfoldidx!=0, f]))\n",
    "    x_val.append(np.array(df_train.loc[df_train.kfoldidx==0, f]))\n",
    "    x_one.append(x_train[-1][0:100,])\n",
    "x_train_sessid = df_train.loc[df_train.kfoldidx!=0].session_id_hash.reset_index(drop=True)\n",
    "x_val_sessid = df_train.loc[df_train.kfoldidx==0].session_id_hash.reset_index(drop=True)\n",
    "\n",
    "y_train = np.array(df_train.loc[df_train.kfoldidx!=0, 'next_sku'])\n",
    "y_val = np.array(df_train.loc[df_train.kfoldidx==0, 'next_sku'])\n",
    "y_one = y_train[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7651082, 6)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_weights =  df_train.loc[df_train.kfoldidx!=0].sample_weights.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "# model architecture\n",
    "class MLP(Model):\n",
    "    def __init__(self, num_sku, num_url, embed_dim=312):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.normal_init = keras.initializers.RandomNormal(mean=0., stddev=0.01)\n",
    "        \n",
    "        self.sku_embed = layers.Embedding(num_sku, embed_dim, self.normal_init)\n",
    "        self.url_embed = layers.Embedding(num_url, embed_dim, self.normal_init)\n",
    "        \n",
    "        self.dense1 = layers.Dense(1024)\n",
    "        self.norm1 = layers.BatchNormalization()\n",
    "        self.activate1 = layers.ReLU()\n",
    "        self.dropout1 = layers.Dropout(0.2)\n",
    "        \n",
    "        self.dense2 = layers.Dense(1024)\n",
    "        self.norm2 = layers.BatchNormalization()\n",
    "        self.activate2 = layers.ReLU()\n",
    "        self.dropout2 = layers.Dropout(0.2)\n",
    "        \n",
    "        self.dense3 = layers.Dense(embed_dim)\n",
    "        self.norm3 = layers.BatchNormalization()\n",
    "        self.activate3 = layers.ReLU(name='sess_embed')\n",
    "        self.dropout3 = layers.Dropout(0.2)\n",
    "        \n",
    "        self.output_bias = tf.random.normal((num_sku,), 0., 0.01)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        lag_sku, lag_url = inputs['sku'], inputs['url']\n",
    "        \n",
    "        sku_embed = layers.Flatten()(self.sku_embed(lag_sku))\n",
    "        url_embed = layers.Flatten()(self.url_embed(lag_url))\n",
    "        \n",
    "        x = layers.concatenate([sku_embed, url_embed])\n",
    "        x = self.activate1(self.norm1(self.dense1(x)))\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.activate2(self.norm2(self.dense2(x)))\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        sess_embed = self.activate3(self.norm3(self.dense3(x)))\n",
    "        x = self.dropout3(sess_embed)\n",
    "        \n",
    "        x = tf.matmul(x, tf.transpose(self.sku_embed.weights[0]))\n",
    "        logits = tf.nn.bias_add(x, self.output_bias, name='logits')\n",
    "\n",
    "        return {'logits': logits, 'embed': sess_embed}\n",
    "    \n",
    "    def build_graph(self):\n",
    "        x = {'sku': Input(shape=(6)), 'url': Input(shape=(6))}\n",
    "        return Model(inputs=x, outputs=self.call(x))\n",
    "    \n",
    "    def predict_subset(self, x, u, l):\n",
    "        _x = []\n",
    "        for i in range(len(x)):\n",
    "            _x.append(x[i][u:l])\n",
    "        return self.predict(_x)\n",
    "    \n",
    "keras.utils.plot_model(MLP(num_sku, num_url).build_graph(), show_shapes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Number of devices: 1\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2000\n",
    "tf_train = tf.data.Dataset.from_tensor_slices(({'sku':x_train[0], 'url':x_train[1]}, y_train, x_train_weights)).batch(batch_size)\n",
    "tf_val = tf.data.Dataset.from_tensor_slices(({'sku':x_val[0], 'url':x_val[1]}, y_val)).batch(batch_size)\n",
    "tf_one = tf.data.Dataset.from_tensor_slices(({'sku':x_one[0], 'url':x_one[1]}, y_one)).batch(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_train = strategy.experimental_distribute_dataset(tf_train)\n",
    "dist_val = strategy.experimental_distribute_dataset(tf_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    model_mlp = MLP(num_sku, num_url)\n",
    "    LossFunc = {'logits':keras.losses.SparseCategoricalCrossentropy(from_logits=True), 'embed':None}\n",
    "    metrics = {'logits': [keras.metrics.SparseCategoricalAccuracy(name='top1_acc'), \n",
    "                      keras.metrics.SparseTopKCategoricalAccuracy(k=20, name='top20_acc')]}\n",
    "    model_mlp.compile(optimizer='adam', loss=LossFunc, metrics=metrics)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3826/3826 [==============================] - 661s 170ms/step - loss: 4.3497 - logits_loss: 4.3497 - logits_top1_acc: 0.6382 - logits_top20_acc: 0.8721 - val_loss: 1.6922 - val_logits_loss: 1.6922 - val_logits_top1_acc: 0.6583 - val_logits_top20_acc: 0.8847\n",
      "\n",
      "Epoch 00001: val_logits_loss improved from inf to 1.69220, saving model to model_update8.h5\n",
      "Epoch 2/10\n",
      "3826/3826 [==============================] - 660s 170ms/step - loss: 3.7887 - logits_loss: 3.7887 - logits_top1_acc: 0.6545 - logits_top20_acc: 0.8927 - val_loss: 1.6494 - val_logits_loss: 1.6494 - val_logits_top1_acc: 0.6641 - val_logits_top20_acc: 0.8894\n",
      "\n",
      "Epoch 00002: val_logits_loss improved from 1.69220 to 1.64941, saving model to model_update8.h5\n",
      "Epoch 3/10\n",
      "3826/3826 [==============================] - 660s 170ms/step - loss: 3.5262 - logits_loss: 3.5262 - logits_top1_acc: 0.6639 - logits_top20_acc: 0.9030 - val_loss: 1.6387 - val_logits_loss: 1.6387 - val_logits_top1_acc: 0.6659 - val_logits_top20_acc: 0.8909\n",
      "\n",
      "Epoch 00003: val_logits_loss improved from 1.64941 to 1.63874, saving model to model_update8.h5\n",
      "Epoch 4/10\n",
      "3826/3826 [==============================] - 659s 169ms/step - loss: 3.3355 - logits_loss: 3.3355 - logits_top1_acc: 0.6718 - logits_top20_acc: 0.9109 - val_loss: 1.6480 - val_logits_loss: 1.6480 - val_logits_top1_acc: 0.6674 - val_logits_top20_acc: 0.8911\n",
      "\n",
      "Epoch 00004: val_logits_loss did not improve from 1.63874\n",
      "Epoch 00004: early stopping\n"
     ]
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='val_logits_loss', mode='min', verbose=1, patience=1)\n",
    "mc = ModelCheckpoint('model_update8.h5', monitor='val_logits_loss', mode='min', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "history = model_mlp.fit(tf_train.shuffle(batch_size), epochs=10, validation_data=tf_val, callbacks=[es, mc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mlp.load_weights('model_update8.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "naid = df_browse.product_sku_hash.cat.categories.tolist().index('_')\n",
    "minorid = df_browse.product_sku_hash.cat.categories.tolist().index('minority')\n",
    "\n",
    "@tf.function\n",
    "def mrr(i, model, x, y, topk=tf.constant(20)):\n",
    "    # this is for tf dataset version\n",
    "    interval = tf.shape(y)[0]\n",
    "    i = tf.cast(i, tf.int32)\n",
    "    _u = interval * i\n",
    "    _l = interval * (i+1)\n",
    "    \n",
    "    y_pred = model(x, training=False)\n",
    "    col_to_zero = [naid, minorid]\n",
    "    tnsr_shape=tf.shape(y_pred['logits'])\n",
    "    mask = [tf.one_hot(col_num*tf.ones((tnsr_shape[0], ), dtype=tf.int32), tnsr_shape[-1]) for col_num in col_to_zero]\n",
    "    mask = tf.reduce_sum(mask, axis=0) * -9999\n",
    "    y_pred['logits'] = tf.add(y_pred['logits'], mask)\n",
    "    \n",
    "    # topk items' id for each session, 2d array\n",
    "    r = tf.math.top_k(y_pred['logits'], k=topk).indices\n",
    "    # True indicate that item is the correct prediction\n",
    "    r = tf.cast(tf.equal(r, tf.expand_dims(tf.cast(y, tf.int32), 1)), tf.float32)\n",
    "    # rank of the correct prediction, rank = 9999999+1 if no correction prediction within topk\n",
    "    r = tf.add((tf.reduce_sum(r, 1)-1) * -9999999, tf.cast(tf.argmax(r, 1) + 1, tf.float32))\n",
    "    return 1/r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58.72942757606506\n",
      "115.52186465263367\n",
      "173.78733730316162\n",
      "MRR=0.7535\n",
      "600733 out of 7651082 records (7.85%) with prediction outside top20\n",
      "---------------------------\n",
      "random pick one per session\n",
      "MRR=0.7902 \n",
      "166926 out of 2157060 sessions (7.74%) with prediction outside top20\n"
     ]
    }
   ],
   "source": [
    "# training MRR\n",
    "\n",
    "s = time.time()\n",
    "rr = []\n",
    "# Iterate over the `tf.distribute.DistributedDataset`\n",
    "i = tf.constant(0)\n",
    "topk = tf.constant(20)\n",
    "for x, y, z in dist_train:\n",
    "    # process dataset elements\n",
    "    rr.append(strategy.run(mrr, args=(i, model_mlp, x, y, topk)))\n",
    "    i += 1\n",
    "    if i % 1000 == 0:\n",
    "        print(time.time()-s)\n",
    "\n",
    "rr = np.append(np.array(rr[0:-1]).reshape(-1), np.array(rr[-1]))\n",
    "print('MRR=%.4f'%np.mean(rr))\n",
    "out_rr = (rr<1/topk.numpy())\n",
    "print('%d out of %d records (%.2f%%) with prediction outside top%d'%(\n",
    "    out_rr.sum(), out_rr.shape[0], (out_rr.mean())*100., topk.numpy()), flush=True)\n",
    "\n",
    "df_train['rr'] = -1\n",
    "df_train['r'] = -1\n",
    "df_train.loc[df_train.kfoldidx!=0, 'rr'] = rr\n",
    "df_train.loc[df_train.kfoldidx!=0, 'r'] = 1/rr\n",
    "\n",
    "print('---------------------------')\n",
    "print('random pick one per session')\n",
    "cond = df_train.loc[df_train.kfoldidx!=0].groupby(['session_id_hash'])['rand_id'].transform(min) == df_train.loc[df_train.kfoldidx!=0, 'rand_id']\n",
    "print('MRR=%.4f ' % np.mean(rr[cond]))\n",
    "print('%d out of %d sessions (%.2f%%) with prediction outside top%d'%(\n",
    "    out_rr[cond].sum(), out_rr[cond].shape[0], (out_rr[cond].mean())*100., topk.numpy()), flush=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR=0.7300\n",
      "206611 out of 1866106 records (11.07%) with prediction outside top20\n",
      "---------------------------\n",
      "random pick one per session\n",
      "MRR=0.7778 \n",
      "52125 out of 526101 sessions (9.91%) with prediction outside top20\n"
     ]
    }
   ],
   "source": [
    "# validation MRR\n",
    "\n",
    "s = time.time()\n",
    "rr = []\n",
    "# Iterate over the `tf.distribute.DistributedDataset`\n",
    "i = tf.constant(0)\n",
    "for x, y in dist_val:\n",
    "    # process dataset elements\n",
    "    rr.append(strategy.run(mrr, args=(i, model_mlp, x, y, topk)))\n",
    "    i += 1\n",
    "    if i % 1000 == 0:\n",
    "        print(time.time()-s)\n",
    "\n",
    "rr = np.append(np.array(rr[0:-1]).reshape(-1), np.array(rr[-1]))\n",
    "print('MRR=%.4f'%np.mean(rr))\n",
    "out_rr = (rr<1/topk.numpy())\n",
    "print('%d out of %d records (%.2f%%) with prediction outside top%d'%(\n",
    "    out_rr.sum(), out_rr.shape[0], (out_rr.mean())*100., topk.numpy()), flush=True)\n",
    "\n",
    "df_train.loc[df_train.kfoldidx==0, 'rr'] = rr\n",
    "df_train.loc[df_train.kfoldidx==0, 'r'] = 1/rr\n",
    "\n",
    "print('---------------------------')\n",
    "print('random pick one per session')\n",
    "cond = df_train.loc[df_train.kfoldidx==0].groupby(['session_id_hash'])['rand_id'].transform(min) == df_train.loc[df_train.kfoldidx==0, 'rand_id']\n",
    "print('MRR=%.4f ' % np.mean(rr[cond]))\n",
    "print('%d out of %d sessions (%.2f%%) with prediction outside top%d'%(\n",
    "    out_rr[cond].sum(), out_rr[cond].shape[0], (out_rr[cond].mean())*100., topk.numpy()), flush=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MRR of testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_browse[df_browse.train==0].copy()\n",
    "df_test = df_test.loc[df_test.next_interacted_sku!=naid]\n",
    "x_test = [np.array(df_test[fea_prod]), np.array(df_test[fea_url])]\n",
    "x_test_sessid = df_test.session_id_hash.reset_index(drop=True)\n",
    "y_test = np.array(df_test.next_interacted_sku)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2000\n",
    "tf_test = tf.data.Dataset.from_tensor_slices(({'sku':x_test[0], 'url':x_test[1]}, y_test)).batch(batch_size)\n",
    "dist_test = strategy.experimental_distribute_dataset(tf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR=0.2830\n",
      "136419 out of 312691 records (43.63%) with prediction outside top20\n",
      "---------------------------\n",
      "random pick one per session\n",
      "MRR=0.3318 \n",
      "18534 out of 52660 sessions (35.20%) with prediction outside top20\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "rr = []\n",
    "# Iterate over the `tf.distribute.DistributedDataset`\n",
    "i = tf.constant(0)\n",
    "topk = tf.constant(20)\n",
    "for x, y in dist_test:\n",
    "    # process dataset elements\n",
    "    rr.append(strategy.run(mrr, args=(i, model_mlp, x, y)))\n",
    "    i += 1\n",
    "    if i % 1000 == 0:\n",
    "        print(time.time()-s)\n",
    "\n",
    "rr = np.append(np.array(rr[0:-1]).reshape(-1), np.array(rr[-1]))\n",
    "print('MRR=%.4f'%np.mean(rr))\n",
    "out_rr = (rr<1/topk.numpy())\n",
    "print('%d out of %d records (%.2f%%) with prediction outside top%d'%(\n",
    "    out_rr.sum(), out_rr.shape[0], (out_rr.mean())*100., topk.numpy()), flush=True)\n",
    "\n",
    "df_test['rr'] = rr\n",
    "df_test['r'] = 1/rr\n",
    "\n",
    "print('---------------------------')\n",
    "print('random pick one per session')\n",
    "cond = df_test.groupby(['session_id_hash'])['rand_id'].transform(min) == df_test['rand_id']\n",
    "print('MRR=%.4f ' % np.mean(rr[cond]))\n",
    "print('%d out of %d sessions (%.2f%%) with prediction outside top%d'%(\n",
    "    out_rr[cond].sum(), out_rr[cond].shape[0], (out_rr[cond].mean())*100., topk.numpy()), flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.r = df_test.r.astype(np.int32)\n",
    "df_test.r_next_sku = df_test.r_next_sku.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_parquet('df_test_update8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = df_browse[df_browse.train==0]\n",
    "df_submission = df_submission.groupby('session_id_hash').tail(1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2000\n",
    "x_submission = [np.array(df_submission[fea_prod]), np.array(df_submission[fea_url])]\n",
    "tf_submission = tf.data.Dataset.from_tensor_slices(({'sku':x_submission[0], 'url':x_submission[1]})).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a6bb2092a124fc1a547431dd20b694c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "naid = df_browse.product_sku_hash.cat.categories.tolist().index('_')\n",
    "minorid = df_browse.product_sku_hash.cat.categories.tolist().index('minority')\n",
    "\n",
    "next_sku_all = []\n",
    "for x in tqdm(tf_submission):\n",
    "    y = model_mlp.predict(x)\n",
    "    y['logits'][:,naid] = -99999\n",
    "    y['logits'][:,minorid] = -99999\n",
    "    next_sku_id = np.argpartition(y['logits'], range(-20, 0), axis=1)[:, ::-1][:,0:20]\n",
    "    next_sku_all += np.array(df_browse.product_sku_hash.cat.categories)[next_sku_id].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_file='./data/rec_test_phase_1.json'\n",
    "with open(test_file) as json_file:\n",
    "    # read the test cases from the provided file\n",
    "    test_queries = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_submission(q):\n",
    "    sess_id = q['query'][0]['session_id_hash']\n",
    "    try:\n",
    "        next_sku = next_sku_all[df_submission.session_id_hash.tolist().index(sess_id)]\n",
    "    except ValueError:\n",
    "        # query with only search events not exists in df_test\n",
    "        next_sku = np.random.choice(df_browse.product_sku_hash.cat.categories, 20, False).tolist()\n",
    "    \n",
    "    # copy the test case\n",
    "    _pred = dict(q)\n",
    "\n",
    "    # append the label - which needs to be a list\n",
    "    _pred[\"label\"] = next_sku\n",
    "    return _pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56114da41f4d4218a5e1e23e0a0902a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/142327 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_predictions = Parallel(n_jobs=multiprocessing.cpu_count()//2, backend='multiprocessing')(delayed(set_submission)(q) for q in tqdm(test_queries))\n",
    "# check for consistency\n",
    "assert len(my_predictions) == len(test_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMAIL = ''\n",
    "local_prediction_file = '{}_{}.json'.format(EMAIL.replace('@', '_'), round(time.time() * 1000))\n",
    "# dump to file\n",
    "with open(local_prediction_file, 'w') as outfile:\n",
    "    json.dump(my_predictions, outfile, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "louiskitlung_connect.hku.hk_1621760806767.json\n"
     ]
    }
   ],
   "source": [
    "print(local_prediction_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "import boto3\n",
    "load_dotenv(verbose=True, dotenv_path='./submission/upload.env')\n",
    "\n",
    "BUCKET_NAME = os.getenv('BUCKET_NAME') # you received it in your e-mail\n",
    "EMAIL = os.getenv('EMAIL') # the e-mail you used to sign up\n",
    "PARTICIPANT_ID = os.getenv('PARTICIPANT_ID') # you received it in your e-mail\n",
    "AWS_ACCESS_KEY = os.getenv('AWS_ACCESS_KEY') # you received it in your e-mail\n",
    "AWS_SECRET_KEY = os.getenv('AWS_SECRET_KEY') # you received it in your e-mail\n",
    "\n",
    "def upload_submission(\n",
    "        local_file: str,\n",
    "        task: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Thanks to Alex Egg for catching the bug!\n",
    "\n",
    "    :param local_file: local path, may be only the file name or a full path\n",
    "    :param task: rec or cart\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Starting submission at {}...\\n\".format(datetime.utcnow()))\n",
    "    # instantiate boto3 client\n",
    "    s3_client = boto3.client(\n",
    "        's3',\n",
    "        aws_access_key_id=AWS_ACCESS_KEY ,\n",
    "        aws_secret_access_key=AWS_SECRET_KEY,\n",
    "        region_name='us-west-2'\n",
    "    )\n",
    "    s3_file_name = os.path.basename(local_file)\n",
    "    # prepare s3 path according to the spec\n",
    "    s3_file_path = '{}/{}/{}'.format(task, PARTICIPANT_ID, s3_file_name)  # it needs to be like e.g. \"rec/id/*.json\"\n",
    "    # upload file\n",
    "    s3_client.upload_file(local_file, BUCKET_NAME, s3_file_path)\n",
    "    # say bye\n",
    "    print(\"\\nAll done at {}: see you, space cowboy!\".format(datetime.utcnow()))\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting submission at 2021-05-23 09:07:00.371677...\n",
      "\n",
      "\n",
      "All done at 2021-05-23 09:07:05.752167: see you, space cowboy!\n"
     ]
    }
   ],
   "source": [
    "upload_submission(local_file=local_prediction_file, task='rec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 17 04:36:37 UTC 2021\r\n"
     ]
    }
   ],
   "source": [
    "!date"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow2_latest_p37] *",
   "language": "python",
   "name": "conda-env-tensorflow2_latest_p37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
