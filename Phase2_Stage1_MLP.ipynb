{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on notebook \"Stage1_MLP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 16 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, activations, losses, Model, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy, SparseTopKCategoricalAccuracy\n",
    "# import tensorflow_ranking as tfr\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os, time, gc, json\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from matplotlib import pyplot as plt\n",
    "from joblib import Parallel, delayed\n",
    "from ast import literal_eval\n",
    "import multiprocessing\n",
    "import itertools\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_shift_features(df, c, lags=[1,2,3,4,5]):\n",
    "    try:\n",
    "        naid = df[c[:-3]].cat.categories.tolist().index('_')\n",
    "    except ValueError:\n",
    "        naid = df[c].max() + 1\n",
    "    \n",
    "    for i in lags:\n",
    "        if (c + '_lag%d'%i) in df.columns:\n",
    "            continue\n",
    "        tmp = df[['session_id_hash', c]].shift(i)\n",
    "        tmp.loc[df.session_id_hash != tmp.session_id_hash, c] = naid   \n",
    "        tmp[c] = tmp[c].astype(df[c].dtype)\n",
    "        df[c + '_lag%d'%i] = tmp[c]\n",
    "        print('Created\\t' + c + '_lag%d'%i)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_browse_phase2 found\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('df_browse_phase2'):\n",
    "    print('df_browse_phase2 not found')\n",
    "    \n",
    "    # read training data\n",
    "    df_browse = pd.read_csv(\"./data/browsing_train.csv\")\n",
    "    df_browse['train'] = np.int8(1)\n",
    "    df_browse.server_timestamp_epoch_ms = pd.to_datetime(df_browse.server_timestamp_epoch_ms, unit='ms')\n",
    "    df_browse['server_day'] = df_browse.server_timestamp_epoch_ms.dt.date\n",
    "    \n",
    "    # read phase 1 testing data\n",
    "    with open('./data/rec_test_phase_1.json') as f:\n",
    "        df_test = json.load(f)\n",
    "    df_test = pd.json_normalize(df_test, record_path =['query'])\n",
    "    df_test['train'] = np.int8(1)\n",
    "    # todo: should take search event into account\n",
    "    df_test = df_test[~df_test.is_search]\n",
    "\n",
    "    df_test.server_timestamp_epoch_ms = pd.to_datetime(df_test.server_timestamp_epoch_ms, unit='ms')\n",
    "    df_test['server_day'] = df_test.server_timestamp_epoch_ms.dt.date\n",
    "\n",
    "    df_test = df_test[df_browse.columns]\n",
    "\n",
    "    # read phase 2 testing data\n",
    "    with open('./data/rec_test_phase_2.json') as f:\n",
    "        df_test2 = json.load(f)\n",
    "    df_test2 = pd.json_normalize(df_test2, record_path =['query'])\n",
    "    df_test2['train'] = np.int8(0)\n",
    "    # todo: should take search event into account\n",
    "    df_test2 = df_test2[~df_test2.is_search]\n",
    "\n",
    "    df_test2.server_timestamp_epoch_ms = pd.to_datetime(df_test2.server_timestamp_epoch_ms, unit='ms')\n",
    "    df_test2['server_day'] = df_test2.server_timestamp_epoch_ms.dt.date\n",
    "\n",
    "    df_test2 = df_test2[df_browse.columns]\n",
    "\n",
    "    # set items which only appears in test set as minority\n",
    "    for c in ['product_sku_hash', 'hashed_url']:\n",
    "        tmp = df_test2[c][~df_test2[c].isna()]\n",
    "        test_only_sku = tmp[~tmp.isin(df_browse[c])]\n",
    "        df_test2.loc[df_test2[c].isin(test_only_sku), c] = 'minority'\n",
    "        del tmp, test_only_sku\n",
    "        gc.collect()\n",
    "        \n",
    "    # combine training and testing data\n",
    "    df_browse = pd.concat((df_browse, df_test, df_test2), ignore_index=True)\n",
    "    df_browse.sort_values(['session_id_hash', 'server_timestamp_epoch_ms'], inplace=True, ignore_index=True)\n",
    "    del df_test, df_test2\n",
    "    gc.collect()\n",
    "    \n",
    "    # combine values with few records in training\n",
    "    for c in ['product_sku_hash', 'hashed_url']:\n",
    "        tmp = df_browse[c].value_counts()\n",
    "        df_browse.loc[df_browse[c].isin(tmp[tmp<=1].index), c] = 'minority'\n",
    "        del tmp\n",
    "        gc.collect()\n",
    "        \n",
    "    # factorize columns\n",
    "    for c in ['product_sku_hash', 'hashed_url']:\n",
    "        # set NaN to _\n",
    "        if df_browse[c].isnull().values.any():\n",
    "            df_browse.loc[df_browse[c].isna(), c] = '_'\n",
    "        # add new column, id start from 0\n",
    "        df_browse[c] = df_browse[c].astype('category')\n",
    "        df_browse[c + '_id'] = df_browse[c].cat.codes\n",
    "        \n",
    "    # add shift features\n",
    "    max_lag = 5\n",
    "    for c in ['product_sku_hash_id', 'hashed_url_id']:\n",
    "        create_shift_features(df_browse, c, lags=[i+1 for i in range(max_lag)])\n",
    "    \n",
    "    # add target features (this column only useful for train set)\n",
    "    create_shift_features(df_browse, 'product_sku_hash_id', lags=[-1])\n",
    "    df_browse.rename(columns={'product_sku_hash_id_lag-1': 'next_sku'}, inplace=True)\n",
    "    \n",
    "    naid = df_browse.product_sku_hash.cat.categories.tolist().index('_')\n",
    "    minorid = df_browse.product_sku_hash.cat.categories.tolist().index('minority')\n",
    "\n",
    "    # add next_interacted_sku\n",
    "    df_browse['next_interacted_sku'] = df_browse.next_sku.copy()\n",
    "    df_browse.loc[df_browse.next_sku==naid, 'next_interacted_sku'] = np.nan\n",
    "    df_browse.next_interacted_sku = df_browse.groupby('session_id_hash')['next_interacted_sku'].apply(lambda x: x.bfill())\n",
    "    df_browse.loc[df_browse.next_interacted_sku.isna(), 'next_interacted_sku'] = naid\n",
    "    gc.collect()\n",
    "    \n",
    "    # set a random id per group, used for calculating testing metric later\n",
    "    df_browse = df_browse.sample(frac=1, random_state=0).reset_index(drop=True)\n",
    "    df_browse['rand_id'] = df_browse.groupby('session_id_hash').cumcount().astype(np.int16)\n",
    "    df_browse.sort_values(['session_id_hash', 'server_timestamp_epoch_ms'], inplace=True, ignore_index=True)\n",
    "    \n",
    "    df_browse.to_parquet('df_browse_phase2')\n",
    "    \n",
    "else:\n",
    "    print('df_browse_phase2 found')\n",
    "    df_browse = pd.read_parquet('df_browse_phase2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_browse['sample_weights'] = pd.to_datetime(df_browse.server_day).dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get num_x from x_lag1 because function create_shift_features\n",
    "# may created a new_id in lag columns\n",
    "num_sku = df_browse.product_sku_hash_id_lag1.max() + 1\n",
    "num_url = df_browse.hashed_url_id_lag1.max() + 1\n",
    "\n",
    "naid = df_browse.product_sku_hash.cat.categories.tolist().index('_')\n",
    "minorid = df_browse.product_sku_hash.cat.categories.tolist().index('minority')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_prod = ['product_sku_hash_id_lag5', 'product_sku_hash_id_lag4', 'product_sku_hash_id_lag3', \n",
    "            'product_sku_hash_id_lag2', 'product_sku_hash_id_lag1', 'product_sku_hash_id']\n",
    "fea_url = ['hashed_url_id_lag5', 'hashed_url_id_lag4', 'hashed_url_id_lag3', \n",
    "           'hashed_url_id_lag2', 'hashed_url_id_lag1', 'hashed_url_id']\n",
    "features = fea_prod + fea_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training + phase 1 testing set\n",
    "df_train = df_browse[df_browse.train==1]\n",
    "df_train = df_train[(df_train.next_sku!=naid)].reset_index(drop=True)\n",
    "\n",
    "# phase 2 testing set\n",
    "df_test = df_browse[df_browse.train==0]\n",
    "df_test = df_test[(df_test.next_sku!=naid)].reset_index(drop=True)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'kfoldidx' in df_train.columns:\n",
    "    df_train.drop(columns='kfoldidx', inplace=True)\n",
    "df_test['kfoldidx'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1906995\n",
      "4    1906207\n",
      "1    1903260\n",
      "3    1901985\n",
      "2    1898741\n",
      "Name: kfoldidx, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def strafiedkfold(df, idcol, k=5):\n",
    "    \"\"\"\n",
    "    sklearn kfold will do unnessary sorting\n",
    "    so build my own function\n",
    "    \"\"\"\n",
    "    df_kidx = df_train.session_id_hash.unique()\n",
    "    np.random.seed(123)\n",
    "    np.random.shuffle(df_kidx)\n",
    "    df_kidx = pd.DataFrame({idcol: df_kidx})\n",
    "    df_kidx['kfoldidx'] = df_kidx.index % k\n",
    "    df = df.merge(df_kidx, on=idcol, copy=False)\n",
    "    print(df['kfoldidx'].value_counts())\n",
    "    return df\n",
    "\n",
    "df_train = strafiedkfold(df_train, 'session_id_hash', k=5)\n",
    "df_train = pd.concat([df_train, df_test])\n",
    "x_train = {}\n",
    "x_val = {}\n",
    "x_one = {}\n",
    "for k, f in zip(['sku','url'], [fea_prod, fea_url]):\n",
    "    x_train[k] = np.array(df_train.loc[df_train.kfoldidx!=0, f])\n",
    "    x_val[k] = np.array(df_train.loc[df_train.kfoldidx==0, f])\n",
    "    x_one[k] = x_train[k][0:100]\n",
    "x_train_sessid = df_train.loc[df_train.kfoldidx!=0].session_id_hash.reset_index(drop=True)\n",
    "x_val_sessid = df_train.loc[df_train.kfoldidx==0].session_id_hash.reset_index(drop=True)\n",
    "\n",
    "y_train = np.array(df_train.loc[df_train.kfoldidx!=0, 'next_sku'])\n",
    "y_val = np.array(df_train.loc[df_train.kfoldidx==0, 'next_sku'])\n",
    "y_one = y_train[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_weights =  df_train.loc[df_train.kfoldidx!=0].sample_weights.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "# model architecture\n",
    "class MLP(Model):\n",
    "    def __init__(self, num_sku, num_url, embed_dim=312):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.normal_init = keras.initializers.RandomNormal(mean=0., stddev=0.01)\n",
    "        \n",
    "        self.sku_embed = layers.Embedding(num_sku, embed_dim, self.normal_init)\n",
    "        self.url_embed = layers.Embedding(num_url, embed_dim, self.normal_init)\n",
    "        \n",
    "        self.dense1 = layers.Dense(1024)\n",
    "        self.norm1 = layers.BatchNormalization()\n",
    "        self.activate1 = layers.ReLU()\n",
    "        self.dropout1 = layers.Dropout(0.2)\n",
    "        \n",
    "        self.dense2 = layers.Dense(1024)\n",
    "        self.norm2 = layers.BatchNormalization()\n",
    "        self.activate2 = layers.ReLU()\n",
    "        self.dropout2 = layers.Dropout(0.2)\n",
    "        \n",
    "        self.dense3 = layers.Dense(embed_dim)\n",
    "        self.norm3 = layers.BatchNormalization()\n",
    "        self.activate3 = layers.ReLU(name='sess_embed')\n",
    "        self.dropout3 = layers.Dropout(0.2)\n",
    "        \n",
    "        self.output_bias = tf.random.normal((num_sku,), 0., 0.01)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        lag_sku, lag_url = inputs['sku'], inputs['url']\n",
    "        \n",
    "        sku_embed = layers.Flatten()(self.sku_embed(lag_sku))\n",
    "        url_embed = layers.Flatten()(self.url_embed(lag_url))\n",
    "        \n",
    "        x = layers.concatenate([sku_embed, url_embed])\n",
    "        x = self.activate1(self.norm1(self.dense1(x)))\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.activate2(self.norm2(self.dense2(x)))\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        sess_embed = self.activate3(self.norm3(self.dense3(x)))\n",
    "        x = self.dropout3(sess_embed)\n",
    "        \n",
    "        x = tf.matmul(x, tf.transpose(self.sku_embed.weights[0]))\n",
    "        logits = tf.nn.bias_add(x, self.output_bias, name='logits')\n",
    "\n",
    "        return {'logits': logits, 'embed': sess_embed}\n",
    "    \n",
    "    def build_graph(self):\n",
    "        x = {'sku': Input(shape=(6)), 'url': Input(shape=(6))}\n",
    "        return Model(inputs=x, outputs=self.call(x))\n",
    "    \n",
    "    def predict_subset(self, x, u, l):\n",
    "        _x = []\n",
    "        for i in range(len(x)):\n",
    "            _x.append(x[i][u:l])\n",
    "        return self.predict(_x)\n",
    "    \n",
    "keras.utils.plot_model(MLP(num_sku, num_url).build_graph(), show_shapes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Number of devices: 1\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2000\n",
    "tf_train = tf.data.Dataset.from_tensor_slices((x_train, y_train, x_train_weights)).batch(batch_size)\n",
    "tf_val = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(batch_size)\n",
    "tf_one = tf.data.Dataset.from_tensor_slices((x_one, y_one)).batch(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_train = strategy.experimental_distribute_dataset(tf_train)\n",
    "dist_val = strategy.experimental_distribute_dataset(tf_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    model_mlp = MLP(num_sku, num_url)\n",
    "    LossFunc = {'logits':keras.losses.SparseCategoricalCrossentropy(from_logits=True), 'embed':None}\n",
    "    metrics = {'logits': [keras.metrics.SparseCategoricalAccuracy(name='top1_acc'), \n",
    "                      keras.metrics.SparseTopKCategoricalAccuracy(k=20, name='top20_acc')]}\n",
    "    model_mlp.compile(optimizer='adam', loss=LossFunc, metrics=metrics)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4008/4008 [==============================] - 705s 172ms/step - loss: 8.9081 - logits_loss: 8.9081 - logits_top1_acc: 0.4681 - logits_top20_acc: 0.6774 - val_loss: 1.8083 - val_logits_loss: 1.8083 - val_logits_top1_acc: 0.6439 - val_logits_top20_acc: 0.8757\n",
      "\n",
      "Epoch 00001: val_logits_loss improved from inf to 1.80827, saving model to model_phase2_nn.h5\n",
      "Epoch 2/10\n",
      "4008/4008 [==============================] - 710s 174ms/step - loss: 4.1429 - logits_loss: 4.1429 - logits_top1_acc: 0.6415 - logits_top20_acc: 0.8819 - val_loss: 1.6982 - val_logits_loss: 1.6982 - val_logits_top1_acc: 0.6540 - val_logits_top20_acc: 0.8849\n",
      "\n",
      "Epoch 00002: val_logits_loss improved from 1.80827 to 1.69820, saving model to model_phase2_nn.h5\n",
      "Epoch 3/10\n",
      "4008/4008 [==============================] - 701s 172ms/step - loss: 3.7808 - logits_loss: 3.7808 - logits_top1_acc: 0.6542 - logits_top20_acc: 0.8962 - val_loss: 1.6709 - val_logits_loss: 1.6709 - val_logits_top1_acc: 0.6582 - val_logits_top20_acc: 0.8879\n",
      "\n",
      "Epoch 00003: val_logits_loss improved from 1.69820 to 1.67088, saving model to model_phase2_nn.h5\n",
      "Epoch 4/10\n",
      "4008/4008 [==============================] - 701s 172ms/step - loss: 3.5414 - logits_loss: 3.5414 - logits_top1_acc: 0.6633 - logits_top20_acc: 0.9052 - val_loss: 1.6749 - val_logits_loss: 1.6749 - val_logits_top1_acc: 0.6593 - val_logits_top20_acc: 0.8882\n",
      "\n",
      "Epoch 00004: val_logits_loss did not improve from 1.67088\n",
      "Epoch 00004: early stopping\n"
     ]
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='val_logits_loss', mode='min', verbose=1, patience=1)\n",
    "mc = ModelCheckpoint('model_phase2_nn.h5', monitor='val_logits_loss', mode='min', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "history = model_mlp.fit(tf_train.shuffle(batch_size), epochs=10, validation_data=tf_val, callbacks=[es, mc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mlp.load_weights('model_phase2_nn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def mrr(i, model, x, y, topk=tf.constant(20)):\n",
    "    # this is for tf dataset version\n",
    "    interval = tf.shape(y)[0]\n",
    "    i = tf.cast(i, tf.int32)\n",
    "    _u = interval * i\n",
    "    _l = interval * (i+1)\n",
    "    \n",
    "    y_pred = model(x, training=False)\n",
    "    col_to_zero = [naid, minorid]\n",
    "    tnsr_shape=tf.shape(y_pred['logits'])\n",
    "    mask = [tf.one_hot(col_num*tf.ones((tnsr_shape[0], ), dtype=tf.int32), tnsr_shape[-1]) for col_num in col_to_zero]\n",
    "    mask = tf.reduce_sum(mask, axis=0) * -9999\n",
    "    y_pred['logits'] = tf.add(y_pred['logits'], mask)\n",
    "    \n",
    "    # topk items' id for each session, 2d array\n",
    "    r = tf.math.top_k(y_pred['logits'], k=topk).indices\n",
    "    # True indicate that item is the correct prediction\n",
    "    r = tf.cast(tf.equal(r, tf.expand_dims(tf.cast(y, tf.int32), 1)), tf.float32)\n",
    "    # rank of the correct prediction, rank = 9999999+1 if no correction prediction within topk\n",
    "    r = tf.add((tf.reduce_sum(r, 1)-1) * -9999999, tf.cast(tf.argmax(r, 1) + 1, tf.float32))\n",
    "    return 1/r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58.8691246509552\n",
      "114.62927651405334\n",
      "171.23737788200378\n",
      "227.02413296699524\n",
      "MRR=0.7323\n",
      "723697 out of 8014076 records (9.03%) with prediction outside top20\n",
      "---------------------------\n",
      "random pick one per session\n",
      "MRR=0.7672 \n",
      "201860 out of 2269052 sessions (8.90%) with prediction outside top20\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "rr = []\n",
    "# Iterate over the `tf.distribute.DistributedDataset`\n",
    "i = tf.constant(0)\n",
    "topk = tf.constant(20)\n",
    "for x, y, z in dist_train:\n",
    "    # process dataset elements\n",
    "    rr.append(strategy.run(mrr, args=(i, model_mlp, x, y, topk)))\n",
    "    i += 1\n",
    "    if i % 1000 == 0:\n",
    "        print(time.time()-s)\n",
    "\n",
    "rr = np.append(np.array(rr[0:-1]).reshape(-1), np.array(rr[-1]))\n",
    "print('MRR=%.4f'%np.mean(rr))\n",
    "out_rr = (rr<1/topk.numpy())\n",
    "print('%d out of %d records (%.2f%%) with prediction outside top%d'%(\n",
    "    out_rr.sum(), out_rr.shape[0], (out_rr.mean())*100., topk.numpy()), flush=True)\n",
    "\n",
    "df_train['rr'] = -1\n",
    "df_train['r'] = -1\n",
    "df_train.loc[df_train.kfoldidx!=0, 'rr'] = rr\n",
    "df_train.loc[df_train.kfoldidx!=0, 'r'] = 1/rr\n",
    "\n",
    "print('---------------------------')\n",
    "print('random pick one per session')\n",
    "cond = df_train.loc[df_train.kfoldidx!=0].groupby(['session_id_hash'])['rand_id'].transform(min) == df_train.loc[df_train.kfoldidx!=0, 'rand_id']\n",
    "print('MRR=%.4f ' % np.mean(rr[cond]))\n",
    "print('%d out of %d sessions (%.2f%%) with prediction outside top%d'%(\n",
    "    out_rr[cond].sum(), out_rr[cond].shape[0], (out_rr[cond].mean())*100., topk.numpy()), flush=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR=0.7236\n",
      "216776 out of 1906995 records (11.37%) with prediction outside top20\n",
      "---------------------------\n",
      "random pick one per session\n",
      "MRR=0.7691 \n",
      "55473 out of 536633 sessions (10.34%) with prediction outside top20\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "rr = []\n",
    "# Iterate over the `tf.distribute.DistributedDataset`\n",
    "i = tf.constant(0)\n",
    "for x, y in dist_val:\n",
    "    # process dataset elements\n",
    "    rr.append(strategy.run(mrr, args=(i, model_mlp, x, y, topk)))\n",
    "    i += 1\n",
    "    if i % 1000 == 0:\n",
    "        print(time.time()-s)\n",
    "\n",
    "rr = np.append(np.array(rr[0:-1]).reshape(-1), np.array(rr[-1]))\n",
    "print('MRR=%.4f'%np.mean(rr))\n",
    "out_rr = (rr<1/topk.numpy())\n",
    "print('%d out of %d records (%.2f%%) with prediction outside top%d'%(\n",
    "    out_rr.sum(), out_rr.shape[0], (out_rr.mean())*100., topk.numpy()), flush=True)\n",
    "\n",
    "df_train.loc[df_train.kfoldidx==0, 'rr'] = rr\n",
    "df_train.loc[df_train.kfoldidx==0, 'r'] = 1/rr\n",
    "\n",
    "print('---------------------------')\n",
    "print('random pick one per session')\n",
    "cond = df_train.loc[df_train.kfoldidx==0].groupby(['session_id_hash'])['rand_id'].transform(min) == df_train.loc[df_train.kfoldidx==0, 'rand_id']\n",
    "print('MRR=%.4f ' % np.mean(rr[cond]))\n",
    "print('%d out of %d sessions (%.2f%%) with prediction outside top%d'%(\n",
    "    out_rr[cond].sum(), out_rr[cond].shape[0], (out_rr[cond].mean())*100., topk.numpy()), flush=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MRR of testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_browse[df_browse.train==0].copy()\n",
    "df_test = df_test.loc[df_test.next_interacted_sku!=naid]\n",
    "x_test = [np.array(df_test[fea_prod]), np.array(df_test[fea_url])]\n",
    "x_test_sessid = df_test.session_id_hash.reset_index(drop=True)\n",
    "y_test = np.array(df_test.next_interacted_sku)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2000\n",
    "tf_test = tf.data.Dataset.from_tensor_slices(({'sku':x_test[0], 'url':x_test[1]}, y_test)).batch(batch_size)\n",
    "dist_test = strategy.experimental_distribute_dataset(tf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR=0.2772\n",
      "325348 out of 729674 records (44.59%) with prediction outside top20\n",
      "---------------------------\n",
      "random pick one per session\n",
      "MRR=0.3315 \n",
      "44010 out of 122524 sessions (35.92%) with prediction outside top20\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "rr = []\n",
    "# Iterate over the `tf.distribute.DistributedDataset`\n",
    "i = tf.constant(0)\n",
    "topk = tf.constant(20)\n",
    "for x, y in dist_test:\n",
    "    # process dataset elements\n",
    "    rr.append(strategy.run(mrr, args=(i, model_mlp, x, y)))\n",
    "    i += 1\n",
    "    if i % 1000 == 0:\n",
    "        print(time.time()-s)\n",
    "\n",
    "rr = np.append(np.array(rr[0:-1]).reshape(-1), np.array(rr[-1]))\n",
    "print('MRR=%.4f'%np.mean(rr))\n",
    "out_rr = (rr<1/topk.numpy())\n",
    "print('%d out of %d records (%.2f%%) with prediction outside top%d'%(\n",
    "    out_rr.sum(), out_rr.shape[0], (out_rr.mean())*100., topk.numpy()), flush=True)\n",
    "\n",
    "df_test['rr'] = rr\n",
    "df_test['r'] = 1/rr\n",
    "\n",
    "print('---------------------------')\n",
    "print('random pick one per session')\n",
    "cond = df_test.groupby(['session_id_hash'])['rand_id'].transform(min) == df_test['rand_id']\n",
    "print('MRR=%.4f ' % np.mean(rr[cond]))\n",
    "print('%d out of %d sessions (%.2f%%) with prediction outside top%d'%(\n",
    "    out_rr[cond].sum(), out_rr[cond].shape[0], (out_rr[cond].mean())*100., topk.numpy()), flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = df_test.loc[df_test.next_sku!=naid]\n",
    "x_test = {'sku': np.array(tmp[fea_prod]), 'url': np.array(tmp[fea_url])}\n",
    "x_test_sessid = tmp.session_id_hash.reset_index(drop=True)\n",
    "y_test = np.array(tmp.next_sku)\n",
    "batch_size = 2000\n",
    "tf_test = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
    "dist_test = strategy.experimental_distribute_dataset(tf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR=0.4376\n",
      "78469 out of 403883 records (19.43%) with prediction outside top20\n",
      "---------------------------\n",
      "random pick one per session\n",
      "MRR=0.4033 \n",
      "28282 out of 122524 sessions (23.08%) with prediction outside top20\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "rr = []\n",
    "# Iterate over the `tf.distribute.DistributedDataset`\n",
    "i = tf.constant(0)\n",
    "topk = tf.constant(20)\n",
    "for x, y in dist_test:\n",
    "    # process dataset elements\n",
    "    rr.append(strategy.run(mrr, args=(i, model_mlp, x, y, topk)))\n",
    "    i += 1\n",
    "    if i % 1000 == 0:\n",
    "        print(time.time()-s)\n",
    "\n",
    "rr = np.append(np.array(rr[0:-1]).reshape(-1), np.array(rr[-1]))\n",
    "print('MRR=%.4f'%np.mean(rr))\n",
    "out_rr = (rr<1/topk.numpy())\n",
    "print('%d out of %d records (%.2f%%) with prediction outside top%d'%(\n",
    "    out_rr.sum(), out_rr.shape[0], (out_rr.mean())*100., topk.numpy()), flush=True)\n",
    "\n",
    "df_test['rr_next_sku'] = -1\n",
    "df_test['r_next_sku'] = -1\n",
    "df_test.loc[df_test.next_sku!=naid, 'rr_next_sku'] = rr\n",
    "df_test.loc[df_test.next_sku!=naid, 'r_next_sku'] = 1/rr\n",
    "\n",
    "print('---------------------------')\n",
    "print('random pick one per session')\n",
    "cond = df_test.loc[df_test.next_sku!=naid].groupby(['session_id_hash'])['rand_id'].transform(min) == df_test.loc[df_test.next_sku!=naid,'rand_id']\n",
    "print('MRR=%.4f ' % np.mean(rr[cond]))\n",
    "print('%d out of %d sessions (%.2f%%) with prediction outside top%d'%(\n",
    "    out_rr[cond].sum(), out_rr[cond].shape[0], (out_rr[cond].mean())*100., topk.numpy()), flush=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = df_browse[df_browse.train==0]\n",
    "df_submission = df_submission.groupby('session_id_hash').tail(1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2000\n",
    "x_submission = [np.array(df_submission[fea_prod]), np.array(df_submission[fea_url])]\n",
    "tf_submission = tf.data.Dataset.from_tensor_slices(({'sku':x_submission[0], 'url':x_submission[1]})).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a6bb2092a124fc1a547431dd20b694c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "naid = df_browse.product_sku_hash.cat.categories.tolist().index('_')\n",
    "minorid = df_browse.product_sku_hash.cat.categories.tolist().index('minority')\n",
    "\n",
    "next_sku_all = []\n",
    "for x in tqdm(tf_submission):\n",
    "    y = model_mlp.predict(x)\n",
    "    y['logits'][:,naid] = -99999\n",
    "    y['logits'][:,minorid] = -99999\n",
    "    next_sku_id = np.argpartition(y['logits'], range(-20, 0), axis=1)[:, ::-1][:,0:20]\n",
    "    next_sku_all += np.array(df_browse.product_sku_hash.cat.categories)[next_sku_id].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_file='./data/rec_test_phase_2.json'\n",
    "with open(test_file) as json_file:\n",
    "    # read the test cases from the provided file\n",
    "    test_queries = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_submission(q):\n",
    "    sess_id = q['query'][0]['session_id_hash']\n",
    "    try:\n",
    "        next_sku = next_sku_all[df_submission.session_id_hash.tolist().index(sess_id)]\n",
    "    except ValueError:\n",
    "        # query with only search events not exists in df_test\n",
    "        next_sku = np.random.choice(df_browse.product_sku_hash.cat.categories, 20, False).tolist()\n",
    "    \n",
    "    # copy the test case\n",
    "    _pred = dict(q)\n",
    "\n",
    "    # append the label - which needs to be a list\n",
    "    _pred[\"label\"] = next_sku\n",
    "    return _pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56114da41f4d4218a5e1e23e0a0902a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/142327 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_predictions = Parallel(n_jobs=multiprocessing.cpu_count()//2, backend='multiprocessing')(delayed(set_submission)(q) for q in tqdm(test_queries))\n",
    "# check for consistency\n",
    "assert len(my_predictions) == len(test_queries)\n",
    "# print out some \"coverage\"\n",
    "# print(\"Predictions made in {} out of {} total test cases\".format(cnt_preds, len(test_queries)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMAIL = ''\n",
    "local_prediction_file = '{}_{}.json'.format(EMAIL.replace('@', '_'), round(time.time() * 1000))\n",
    "# dump to file\n",
    "with open(local_prediction_file, 'w') as outfile:\n",
    "    json.dump(my_predictions, outfile, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "louiskitlung_connect.hku.hk_1621760806767.json\n"
     ]
    }
   ],
   "source": [
    "print(local_prediction_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "import boto3\n",
    "load_dotenv(verbose=True, dotenv_path='./submission/upload.env')\n",
    "\n",
    "BUCKET_NAME = os.getenv('BUCKET_NAME') # you received it in your e-mail\n",
    "EMAIL = os.getenv('EMAIL') # the e-mail you used to sign up\n",
    "PARTICIPANT_ID = os.getenv('PARTICIPANT_ID') # you received it in your e-mail\n",
    "AWS_ACCESS_KEY = os.getenv('AWS_ACCESS_KEY') # you received it in your e-mail\n",
    "AWS_SECRET_KEY = os.getenv('AWS_SECRET_KEY') # you received it in your e-mail\n",
    "\n",
    "def upload_submission(\n",
    "        local_file: str,\n",
    "        task: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Thanks to Alex Egg for catching the bug!\n",
    "\n",
    "    :param local_file: local path, may be only the file name or a full path\n",
    "    :param task: rec or cart\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Starting submission at {}...\\n\".format(datetime.utcnow()))\n",
    "    # instantiate boto3 client\n",
    "    s3_client = boto3.client(\n",
    "        's3',\n",
    "        aws_access_key_id=AWS_ACCESS_KEY ,\n",
    "        aws_secret_access_key=AWS_SECRET_KEY,\n",
    "        region_name='us-west-2'\n",
    "    )\n",
    "    s3_file_name = os.path.basename(local_file)\n",
    "    # prepare s3 path according to the spec\n",
    "    s3_file_path = '{}/{}/{}'.format(task, PARTICIPANT_ID, s3_file_name)  # it needs to be like e.g. \"rec/id/*.json\"\n",
    "    # upload file\n",
    "    s3_client.upload_file(local_file, BUCKET_NAME, s3_file_path)\n",
    "    # say bye\n",
    "    print(\"\\nAll done at {}: see you, space cowboy!\".format(datetime.utcnow()))\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting submission at 2021-05-23 09:07:00.371677...\n",
      "\n",
      "\n",
      "All done at 2021-05-23 09:07:05.752167: see you, space cowboy!\n"
     ]
    }
   ],
   "source": [
    "upload_submission(local_file=local_prediction_file, task='rec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 17 04:36:37 UTC 2021\r\n"
     ]
    }
   ],
   "source": [
    "!date"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow2_latest_p37] *",
   "language": "python",
   "name": "conda-env-tensorflow2_latest_p37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
